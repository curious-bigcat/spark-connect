{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.13 (main, Mar 23 2025, 17:50:53) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "Platform: darwin\n",
      "Machine: arm64\n",
      "Processor: arm\n",
      "Architecture: ('64bit', '')\n",
      "System: Darwin\n",
      "Platform details: macOS-15.3.2-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print(f\"System: {platform.system()}\")\n",
    "print(f\"Platform details: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./snowpark_connect-0.4.0-py3-none-any.whl\n",
      "Collecting fsspec[http]\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting snowflake-core<2,>=1.0.5\n",
      "  Using cached snowflake_core-1.2.0-py3-none-any.whl (2.0 MB)\n",
      "Collecting grpcio-tools>=1.48.1\n",
      "  Using cached grpcio_tools-1.71.0-cp310-cp310-macosx_12_0_universal2.whl (5.9 MB)\n",
      "Collecting sqlglot>=26.4.0\n",
      "  Downloading sqlglot-26.12.1-py3-none-any.whl (454 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting snowflake-snowpark-python[pandas]>=1.29.1\n",
      "  Using cached snowflake_snowpark_python-1.30.0-py3-none-any.whl (1.6 MB)\n",
      "Collecting JayDeBeApi\n",
      "  Using cached JayDeBeApi-1.2.3-py3-none-any.whl (26 kB)\n",
      "Collecting numpy<2,>=1.15\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting grpcio>=1.48.1\n",
      "  Using cached grpcio-1.71.0-cp310-cp310-macosx_12_0_universal2.whl (11.3 MB)\n",
      "Collecting pandas>=2.2.2\n",
      "  Using cached pandas-2.2.3-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Collecting protobuf>5.29\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Collecting pyspark<4,>=3.5.0\n",
      "  Using cached pyspark-3.5.5-py2.py3-none-any.whl\n",
      "Collecting lxml\n",
      "  Using cached lxml-5.3.1-cp310-cp310-macosx_10_9_universal2.whl (8.1 MB)\n",
      "Collecting grpcio_status>=1.48.1\n",
      "  Using cached grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Collecting tzlocal\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2025.3.2-py3-none-any.whl (30 kB)\n",
      "Collecting jpype1\n",
      "  Using cached jpype1-1.5.2-cp310-cp310-macosx_10_9_universal2.whl (584 kB)\n",
      "Collecting protobuf>5.29\n",
      "  Using cached protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Collecting googleapis-common-protos>=1.5.5\n",
      "  Using cached googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (171 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting pydantic>=2\n",
      "  Using cached pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Collecting atpublic>=4\n",
      "  Using cached atpublic-5.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting urllib3\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Collecting snowflake-connector-python\n",
      "  Using cached snowflake_connector_python-3.14.0-cp310-cp310-macosx_11_0_arm64.whl (963 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.1.0\n",
      "  Using cached typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-macosx_11_0_arm64.whl (455 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting packaging\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4\n",
      "  Using cached aiobotocore-2.21.1-py3-none-any.whl (78 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1\n",
      "  Using cached aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.37.2,>=1.37.0\n",
      "  Using cached botocore-1.37.1-py3-none-any.whl (13.4 MB)\n",
      "Collecting multidict<7.0.0,>=6.0.0\n",
      "  Downloading multidict-6.3.1-cp310-cp310-macosx_11_0_arm64.whl (36 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl (92 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core==2.33.0\n",
      "  Using cached pydantic_core-2.33.0-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sortedcontainers>=2.4.0\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Collecting cryptography>=3.1.0\n",
      "  Using cached cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "Collecting pyjwt<3.0.0\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Collecting filelock<4,>=3.5\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0\n",
      "  Using cached pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
      "Collecting platformdirs<5.0.0,>=2.6.0\n",
      "  Using cached platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
      "Collecting cffi<2.0.0,>=1.9\n",
      "  Using cached cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl (178 kB)\n",
      "Collecting tomlkit\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.1-cp310-cp310-macosx_10_9_universal2.whl (198 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting pyarrow<19.0.0\n",
      "  Using cached pyarrow-18.1.0-cp310-cp310-macosx_12_0_arm64.whl (29.5 MB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, py4j, asn1crypto, wrapt, wheel, urllib3, tzlocal, tzdata, typing-extensions, tomlkit, sqlglot, six, setuptools, pyyaml, pyspark, pyjwt, pycparser, pyarrow, protobuf, propcache, platformdirs, packaging, numpy, lxml, jmespath, idna, grpcio, fsspec, frozenlist, filelock, cloudpickle, charset_normalizer, certifi, attrs, atpublic, async-timeout, annotated-types, aioitertools, aiohappyeyeballs, typing-inspection, requests, python-dateutil, pydantic-core, multidict, jpype1, grpcio-tools, googleapis-common-protos, cffi, aiosignal, yarl, pydantic, pandas, JayDeBeApi, grpcio_status, cryptography, botocore, pyOpenSSL, aiohttp, snowflake-connector-python, aiobotocore, snowflake-snowpark-python, snowflake-core, s3fs, snowpark-connect\n",
      "  Attempting uninstall: sortedcontainers\n",
      "    Found existing installation: sortedcontainers 2.4.0\n",
      "    Uninstalling sortedcontainers-2.4.0:\n",
      "      Successfully uninstalled sortedcontainers-2.4.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: asn1crypto\n",
      "    Found existing installation: asn1crypto 1.5.1\n",
      "    Uninstalling asn1crypto-1.5.1:\n",
      "      Successfully uninstalled asn1crypto-1.5.1\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.45.1\n",
      "    Uninstalling wheel-0.45.1:\n",
      "      Successfully uninstalled wheel-0.45.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: tzlocal\n",
      "    Found existing installation: tzlocal 5.3.1\n",
      "    Uninstalling tzlocal-5.3.1:\n",
      "      Successfully uninstalled tzlocal-5.3.1\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.0\n",
      "    Uninstalling typing_extensions-4.13.0:\n",
      "      Successfully uninstalled typing_extensions-4.13.0\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.13.2\n",
      "    Uninstalling tomlkit-0.13.2:\n",
      "      Successfully uninstalled tomlkit-0.13.2\n",
      "  Attempting uninstall: sqlglot\n",
      "    Found existing installation: sqlglot 26.12.0\n",
      "    Uninstalling sqlglot-26.12.0:\n",
      "      Successfully uninstalled sqlglot-26.12.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 78.1.0\n",
      "    Uninstalling setuptools-78.1.0:\n",
      "      Successfully uninstalled setuptools-78.1.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.5\n",
      "    Uninstalling pyspark-3.5.5:\n",
      "      Successfully uninstalled pyspark-3.5.5\n",
      "  Attempting uninstall: pyjwt\n",
      "    Found existing installation: PyJWT 2.10.1\n",
      "    Uninstalling PyJWT-2.10.1:\n",
      "      Successfully uninstalled PyJWT-2.10.1\n",
      "  Attempting uninstall: pycparser\n",
      "    Found existing installation: pycparser 2.22\n",
      "    Uninstalling pycparser-2.22:\n",
      "      Successfully uninstalled pycparser-2.22\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: propcache\n",
      "    Found existing installation: propcache 0.3.1\n",
      "    Uninstalling propcache-0.3.1:\n",
      "      Successfully uninstalled propcache-0.3.1\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.3.7\n",
      "    Uninstalling platformdirs-4.3.7:\n",
      "      Successfully uninstalled platformdirs-4.3.7\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.3.1\n",
      "    Uninstalling lxml-5.3.1:\n",
      "      Successfully uninstalled lxml-5.3.1\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.5.0\n",
      "    Uninstalling frozenlist-1.5.0:\n",
      "      Successfully uninstalled frozenlist-1.5.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.1\n",
      "    Uninstalling charset-normalizer-3.4.1:\n",
      "      Successfully uninstalled charset-normalizer-3.4.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.1.31\n",
      "    Uninstalling certifi-2025.1.31:\n",
      "      Successfully uninstalled certifi-2025.1.31\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 25.3.0\n",
      "    Uninstalling attrs-25.3.0:\n",
      "      Successfully uninstalled attrs-25.3.0\n",
      "  Attempting uninstall: atpublic\n",
      "    Found existing installation: atpublic 5.1\n",
      "    Uninstalling atpublic-5.1:\n",
      "      Successfully uninstalled atpublic-5.1\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: aioitertools\n",
      "    Found existing installation: aioitertools 0.12.0\n",
      "    Uninstalling aioitertools-0.12.0:\n",
      "      Successfully uninstalled aioitertools-0.12.0\n",
      "  Attempting uninstall: aiohappyeyeballs\n",
      "    Found existing installation: aiohappyeyeballs 2.6.1\n",
      "    Uninstalling aiohappyeyeballs-2.6.1:\n",
      "      Successfully uninstalled aiohappyeyeballs-2.6.1\n",
      "  Attempting uninstall: typing-inspection\n",
      "    Found existing installation: typing-inspection 0.4.0\n",
      "    Uninstalling typing-inspection-0.4.0:\n",
      "      Successfully uninstalled typing-inspection-0.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.0\n",
      "    Uninstalling pydantic_core-2.33.0:\n",
      "      Successfully uninstalled pydantic_core-2.33.0\n",
      "  Attempting uninstall: multidict\n",
      "    Found existing installation: multidict 6.2.0\n",
      "    Uninstalling multidict-6.2.0:\n",
      "      Successfully uninstalled multidict-6.2.0\n",
      "  Attempting uninstall: jpype1\n",
      "    Found existing installation: jpype1 1.5.2\n",
      "    Uninstalling jpype1-1.5.2:\n",
      "      Successfully uninstalled jpype1-1.5.2\n",
      "  Attempting uninstall: grpcio-tools\n",
      "    Found existing installation: grpcio-tools 1.71.0\n",
      "    Uninstalling grpcio-tools-1.71.0:\n",
      "      Successfully uninstalled grpcio-tools-1.71.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.69.2\n",
      "    Uninstalling googleapis-common-protos-1.69.2:\n",
      "      Successfully uninstalled googleapis-common-protos-1.69.2\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.17.1\n",
      "    Uninstalling cffi-1.17.1:\n",
      "      Successfully uninstalled cffi-1.17.1\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.2\n",
      "    Uninstalling aiosignal-1.3.2:\n",
      "      Successfully uninstalled aiosignal-1.3.2\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.18.3\n",
      "    Uninstalling yarl-1.18.3:\n",
      "      Successfully uninstalled yarl-1.18.3\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.1\n",
      "    Uninstalling pydantic-2.11.1:\n",
      "      Successfully uninstalled pydantic-2.11.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: JayDeBeApi\n",
      "    Found existing installation: JayDeBeApi 1.2.3\n",
      "    Uninstalling JayDeBeApi-1.2.3:\n",
      "      Successfully uninstalled JayDeBeApi-1.2.3\n",
      "  Attempting uninstall: grpcio_status\n",
      "    Found existing installation: grpcio-status 1.71.0\n",
      "    Uninstalling grpcio-status-1.71.0:\n",
      "      Successfully uninstalled grpcio-status-1.71.0\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 44.0.2\n",
      "    Uninstalling cryptography-44.0.2:\n",
      "      Successfully uninstalled cryptography-44.0.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.37.1\n",
      "    Uninstalling botocore-1.37.1:\n",
      "      Successfully uninstalled botocore-1.37.1\n",
      "  Attempting uninstall: pyOpenSSL\n",
      "    Found existing installation: pyOpenSSL 25.0.0\n",
      "    Uninstalling pyOpenSSL-25.0.0:\n",
      "      Successfully uninstalled pyOpenSSL-25.0.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.14\n",
      "    Uninstalling aiohttp-3.11.14:\n",
      "      Successfully uninstalled aiohttp-3.11.14\n",
      "  Attempting uninstall: snowflake-connector-python\n",
      "    Found existing installation: snowflake-connector-python 3.14.0\n",
      "    Uninstalling snowflake-connector-python-3.14.0:\n",
      "      Successfully uninstalled snowflake-connector-python-3.14.0\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 2.21.1\n",
      "    Uninstalling aiobotocore-2.21.1:\n",
      "      Successfully uninstalled aiobotocore-2.21.1\n",
      "  Attempting uninstall: snowflake-snowpark-python\n",
      "    Found existing installation: snowflake-snowpark-python 1.30.0\n",
      "    Uninstalling snowflake-snowpark-python-1.30.0:\n",
      "      Successfully uninstalled snowflake-snowpark-python-1.30.0\n",
      "  Attempting uninstall: snowflake-core\n",
      "    Found existing installation: snowflake.core 1.2.0\n",
      "    Uninstalling snowflake.core-1.2.0:\n",
      "      Successfully uninstalled snowflake.core-1.2.0\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2025.3.0\n",
      "    Uninstalling s3fs-2025.3.0:\n",
      "      Successfully uninstalled s3fs-2025.3.0\n",
      "  Attempting uninstall: snowpark-connect\n",
      "    Found existing installation: snowpark_connect 0.4.0\n",
      "    Uninstalling snowpark_connect-0.4.0:\n",
      "      Successfully uninstalled snowpark_connect-0.4.0\n",
      "Successfully installed JayDeBeApi-1.2.3 aiobotocore-2.21.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aioitertools-0.12.0 aiosignal-1.3.2 annotated-types-0.7.0 asn1crypto-1.5.1 async-timeout-5.0.1 atpublic-5.1 attrs-25.3.0 botocore-1.37.1 certifi-2025.1.31 cffi-1.17.1 charset_normalizer-3.4.1 cloudpickle-3.0.0 cryptography-44.0.2 filelock-3.18.0 frozenlist-1.5.0 fsspec-2025.3.2 googleapis-common-protos-1.69.2 grpcio-1.71.0 grpcio-tools-1.71.0 grpcio_status-1.71.0 idna-3.10 jmespath-1.0.1 jpype1-1.5.2 lxml-5.3.1 multidict-6.3.1 numpy-1.26.4 packaging-24.2 pandas-2.2.3 platformdirs-4.3.7 propcache-0.3.1 protobuf-5.29.4 py4j-0.10.9.7 pyOpenSSL-25.0.0 pyarrow-18.1.0 pycparser-2.22 pydantic-2.11.1 pydantic-core-2.33.0 pyjwt-2.10.1 pyspark-3.5.5 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 requests-2.32.3 s3fs-2025.3.2 setuptools-78.1.0 six-1.17.0 snowflake-connector-python-3.14.0 snowflake-core-1.2.0 snowflake-snowpark-python-1.30.0 snowpark-connect-0.4.0 sortedcontainers-2.4.0 sqlglot-26.12.1 tomlkit-0.13.2 typing-extensions-4.13.0 typing-inspection-0.4.0 tzdata-2025.2 tzlocal-5.3.1 urllib3-2.3.0 wheel-0.45.1 wrapt-1.17.2 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U --force-reinstall snowpark_connect-0.4.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake import snowpark_connect\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n",
    "os.environ[\"SPARK_REMOTE\"] = \"sc://localhost:15002\"\n",
    "\n",
    "snowpark_connect.start_session(remote_url=\"sc://localhost:15002\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NotebookSnowparkConnect\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|  2.0|\n",
      "|  2|  3.5|\n",
      "|  3|  4.1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from snowflake import snowpark_connect\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n",
    "os.environ[\"SPARK_REMOTE\"] = \"sc://localhost:15002\"\n",
    "\n",
    "snowpark_connect.start_session(remote_url=\"sc://localhost:15002\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NotebookSnowparkConnect\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1, 2.0), (2, 3.5), (3, 4.1)], [\"id\", \"value\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Snowpark Connect + PySpark Notebook Template\n",
    "import os\n",
    "from snowflake import snowpark_connect\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# üîå Enable Spark Connect mode\n",
    "os.environ[\"SPARK_CONNECT_MODE_ENABLED\"] = \"1\"\n",
    "os.environ[\"SPARK_REMOTE\"] = \"sc://localhost:15002\"\n",
    "\n",
    "# üöÄ Start Snowpark Connect session (must have snowpark-connect running locally)\n",
    "snowpark_connect.start_session(remote_url=\"sc://localhost:15002\")\n",
    "\n",
    "# üî• Create SparkSession (backed by Snowflake compute)\n",
    "spark = SparkSession.builder.appName(\"SnowparkConnectNotebook\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+----------+---------+----------+-------------+------------------+--------+-------------------+-------------+----+----------+--------------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+\n",
      "|index|           order_id|order_date|   status|fulfilment|sales_channel|ship_service_level|   style|                sku|     category|size|      asin|      courier_status|qty|currency|amount|  ship_city|    ship_state|ship_postal_code|ship_country|       promotion_ids|  b2b|fulfilled_by|\n",
      "+-----+-------------------+----------+---------+----------+-------------+------------------+--------+-------------------+-------------+----+----------+--------------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+\n",
      "|    0|405-8078784-5731545|0022-04-30|     NULL|  Merchant|    Amazon.in|          Standard|  SET389|     SET389-KR-NP-S|          Set|   S|B09KXVBD7Z|           Cancelled|  0|     INR|647.62|     MUMBAI|   MAHARASHTRA|          400081|          IN|                NULL|false|   Easy Ship|\n",
      "|    1|171-9198151-1101146|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard| JNE3781|    JNE3781-KR-XXXL|        kurta| 3XL|B09K3WFS32|Shipped - Deliver...|  1|     INR| 406.0|  BENGALURU|     KARNATAKA|          560085|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|    2|404-0687676-7273146|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3371|      JNE3371-KR-XL|        kurta|  XL|B07WV4JV4D|             Shipped|  1|     INR| 329.0|NAVI MUMBAI|   MAHARASHTRA|          410210|          IN|IN Core Free Ship...| true|        NULL|\n",
      "|    3|403-9615377-8133951|0022-04-30|     NULL|  Merchant|    Amazon.in|          Standard|   J0341|         J0341-DR-L|Western Dress|   L|B099NRCT7B|           Cancelled|  0|     INR|753.33| PUDUCHERRY|    PUDUCHERRY|          605008|          IN|                NULL|false|   Easy Ship|\n",
      "|    4|407-1069790-7240320|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3671|    JNE3671-TU-XXXL|          Top| 3XL|B098714BZP|             Shipped|  1|     INR| 574.0|    CHENNAI|    TAMIL NADU|          600073|          IN|                NULL|false|        NULL|\n",
      "|    5|404-1490984-4578765|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited|  SET264|    SET264-KR-NP-XL|          Set|  XL|B08YN7XDSG|             Shipped|  1|     INR| 824.0|  GHAZIABAD| UTTAR PRADESH|          201102|          IN|IN Core Free Ship...|false|        NULL|\n",
      "|    6|408-5748499-6859555|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited|   J0095|        J0095-SET-L|          Set|   L|B08CMHNWBN|             Shipped|  1|     INR| 653.0| CHANDIGARH|    CHANDIGARH|          160036|          IN|IN Core Free Ship...|false|        NULL|\n",
      "|    7|406-7807733-3785945|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard| JNE3405|       JNE3405-KR-S|        kurta|   S|B081WX4G4Q|Shipped - Deliver...|  1|     INR| 399.0|  HYDERABAD|     TELANGANA|          500032|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|    8|407-5443024-5233168|0022-04-30|Cancelled|    Amazon|    Amazon.in|         Expedited|  SET200|SET200-KR-NP-A-XXXL|          Set| 3XL|B08L91ZZXN|           Cancelled|  0|    NULL|  NULL|  HYDERABAD|     TELANGANA|          500008|          IN|IN Core Free Ship...|false|        NULL|\n",
      "|    9|402-4393761-0311520|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3461|     JNE3461-KR-XXL|        kurta| XXL|B08B3XF5MH|             Shipped|  1|     INR| 363.0|    Chennai|    TAMIL NADU|          600041|          IN|                NULL|false|        NULL|\n",
      "|   10|407-5633625-6970741|0022-04-30|  Shipped|    Amazon|    Amazon.in|          Standard| JNE3160|     JNE3160-KR-G-S|        kurta|   S|B07K3YQLF1|             Shipped|  1|     INR| 685.0|    CHENNAI|    TAMIL NADU|          600073|          IN|                NULL|false|        NULL|\n",
      "|   11|171-4638481-6326716|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3500|      JNE3500-KR-XS|        kurta|  XS|B098117DJ3|             Shipped|  1|     INR| 364.0|      NOIDA| UTTAR PRADESH|          201303|          IN|                NULL|false|        NULL|\n",
      "|   12|405-5513694-8146768|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard| JNE3405|      JNE3405-KR-XS|        kurta|  XS|B081XCMYXJ|Shipped - Deliver...|  1|     INR| 399.0|  Amravati.|   MAHARASHTRA|          444606|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|   13|408-7955685-3083534|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited|  SET182|    SET182-KR-DH-XS|          Set|  XS|B085HS947T|             Shipped|  1|     INR| 657.0|     MUMBAI|   MAHARASHTRA|          400053|          IN|                NULL|false|        NULL|\n",
      "|   14|408-1298370-1920302|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard|   J0351|        J0351-SET-L|          Set|   L|B09CSSQY4F|Shipped - Deliver...|  1|     INR| 771.0|     MUMBAI|   MAHARASHTRA|          400053|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|   15|403-4965581-9520319|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard|PJNE3368|    PJNE3368-KR-6XL|        kurta| 6XL|B09PY99SVJ|Shipped - Deliver...|  1|     INR| 544.0|   GUNTAKAL|ANDHRA PRADESH|          515801|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|   16|406-9379318-6555504|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3721|     JNE3721-KR-XXL|        kurta| XXL|B099FCT65D|             Shipped|  1|     INR| 329.0|     JAIPUR|     RAJASTHAN|          302020|          IN|IN Core Free Ship...|false|        NULL|\n",
      "|   17|405-9013803-8009918|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited| JNE3405|      JNE3405-KR-XL|        kurta|  XL|B081WT6GG7|             Shipped|  1|     INR| 399.0|  NEW DELHI|         DELHI|          110074|          IN|                NULL|false|        NULL|\n",
      "|   18|402-4030358-5835511|0022-04-30|  Shipped|  Merchant|    Amazon.in|          Standard| JNE3697|     JNE3697-KR-XXL|        kurta| XXL|B098133PV5|Shipped - Deliver...|  1|     INR| 458.0|    Gurgaon|       HARYANA|          122004|          IN|Amazon PLCC Free-...|false|   Easy Ship|\n",
      "|   19|405-5957858-1051546|0022-04-30|  Shipped|    Amazon|    Amazon.in|         Expedited|  SET254|    SET254-KR-NP-XS|          Set|  XS|B0983DDPL6|             Shipped|  1|     INR| 886.0|  BENGALURU|     KARNATAKA|          560017|          IN|                NULL|false|        NULL|\n",
      "+-----+-------------------+----------+---------+----------+-------------+------------------+--------+-------------------+-------------+----+----------+--------------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf_df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "sf_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----+----------+-----------+-----------+------------------+-----------+-------------+\n",
      "|SHIP_CITY|CATEGORY|SIZE|day       |order_count|total_sales|avg_order_value   |total_items|unique_orders|\n",
      "+---------+--------+----+----------+-----------+-----------+------------------+-----------+-------------+\n",
      "|BENGALURU|Set     |M   |2022-06-01|14         |12566.0    |897.5714285714286 |14         |14           |\n",
      "|BENGALURU|Set     |M   |0022-04-20|14         |12264.0    |876.0             |14         |13           |\n",
      "|HYDERABAD|Set     |3XL |2022-05-07|13         |11663.0    |897.1538461538462 |13         |11           |\n",
      "|BENGALURU|Set     |S   |0022-04-14|13         |11508.0    |885.2307692307693 |13         |13           |\n",
      "|NEW DELHI|Set     |3XL |2022-04-02|14         |11487.0    |820.5             |14         |12           |\n",
      "|BENGALURU|Set     |L   |2022-06-01|13         |11456.0    |881.2307692307693 |14         |13           |\n",
      "|MUMBAI   |Set     |3XL |2022-06-01|12         |11364.0    |947.0             |12         |9            |\n",
      "|BENGALURU|Set     |L   |0022-04-16|13         |11216.0    |862.7692307692307 |13         |12           |\n",
      "|BENGALURU|Set     |S   |2022-05-01|13         |11140.0    |856.9230769230769 |13         |13           |\n",
      "|HYDERABAD|Set     |M   |0022-05-30|11         |11069.0    |1006.2727272727273|11         |10           |\n",
      "|BENGALURU|Set     |M   |0022-04-14|13         |10964.0    |843.3846153846154 |13         |12           |\n",
      "|BENGALURU|Set     |XL  |2022-05-03|13         |10880.0    |836.9230769230769 |13         |13           |\n",
      "|BENGALURU|Set     |L   |0022-04-28|14         |10829.0    |773.5             |14         |10           |\n",
      "|MUMBAI   |Set     |3XL |2022-05-06|12         |10699.0    |891.5833333333334 |12         |10           |\n",
      "|ARAKONAM |Saree   |Free|0022-04-16|15         |10687.0    |712.4666666666667 |15         |2            |\n",
      "|BENGALURU|Set     |XL  |0022-05-22|12         |10620.0    |885.0             |12         |12           |\n",
      "|BENGALURU|Set     |M   |2022-04-11|12         |10539.0    |878.25            |12         |11           |\n",
      "|BENGALURU|Set     |L   |0022-06-21|13         |10529.0    |809.9230769230769 |13         |12           |\n",
      "|HYDERABAD|Set     |XL  |2022-06-11|9          |10457.0    |1161.888888888889 |9          |8            |\n",
      "|BENGALURU|Set     |M   |2022-05-04|10         |10447.0    |1044.7            |10         |10           |\n",
      "+---------+--------+----+----------+-----------+-----------+------------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# üîπ Read from Snowflake table\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "# üîπ Cast numeric types (in case they come in as strings from CSV loads)\n",
    "df = (\n",
    "    df.withColumn(\"AMOUNT\", F.col(\"AMOUNT\").cast(\"double\"))\n",
    "      .withColumn(\"QTY\", F.col(\"QTY\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# üîπ Filter: only shipped orders\n",
    "shipped_df = df.filter(F.col(\"STATUS\").like(\"%Shipped%\"))\n",
    "\n",
    "# üîπ Define window: top 3 orders per ship_city\n",
    "window_spec = Window.partitionBy(\"SHIP_CITY\").orderBy(F.desc(\"AMOUNT\"))\n",
    "\n",
    "# üîπ Perform group-based aggregations\n",
    "result_df = (\n",
    "    shipped_df.withColumn(\"rank_in_city\", F.row_number().over(window_spec))\n",
    "              .withColumn(\"day\", F.date_format(\"ORDER_DATE\", \"yyyy-MM-dd\"))\n",
    "              .groupBy(\"SHIP_CITY\", \"CATEGORY\", \"SIZE\", \"day\")\n",
    "              .agg(\n",
    "                  F.count(\"*\").alias(\"order_count\"),\n",
    "                  F.sum(\"AMOUNT\").alias(\"total_sales\"),\n",
    "                  F.avg(\"AMOUNT\").alias(\"avg_order_value\"),\n",
    "                  F.sum(\"QTY\").alias(\"total_items\"),\n",
    "                  F.countDistinct(\"ORDER_ID\").alias(\"unique_orders\")\n",
    "              )\n",
    "              .orderBy(F.desc(\"total_sales\"))\n",
    ")\n",
    "\n",
    "# üîπ Show results\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----------------+\n",
      "|SHIP_STATE    |avg_top_ranked_amt|unique_top_orders|\n",
      "+--------------+------------------+-----------------+\n",
      "|ANDHRA PRADESH|5584.0            |1                |\n",
      "|PUNJAB        |5495.0            |1                |\n",
      "|RAJASTHAN     |3416.86           |2                |\n",
      "|UTTAR PRADESH |2948.0            |2                |\n",
      "|KARNATAKA     |2864.0            |1                |\n",
      "|HARYANA       |2796.0            |1                |\n",
      "|WEST BENGAL   |2698.0            |1                |\n",
      "|MAHARASHTRA   |2672.0            |4                |\n",
      "+--------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "# Apply type casting (optional depending on schema)\n",
    "df = df.withColumn(\"AMOUNT\", F.col(\"AMOUNT\").cast(\"double\"))\n",
    "\n",
    "# Define windows\n",
    "w1 = Window.partitionBy(\"FULFILLED_BY\").orderBy(F.desc(\"AMOUNT\"))\n",
    "w2 = Window.partitionBy(\"SHIP_CITY\", \"CATEGORY\", \"SIZE\").orderBy(F.desc(\"AMOUNT\"))\n",
    "\n",
    "df_ranked = df.withColumn(\"rank_by_fulfillment\", F.rank().over(w1)) \\\n",
    "              .withColumn(\"rank_by_combo\", F.dense_rank().over(w2))\n",
    "\n",
    "result2 = df_ranked.filter((F.col(\"rank_by_fulfillment\") <= 5) & (F.col(\"rank_by_combo\") <= 3)) \\\n",
    "                   .groupBy(\"SHIP_STATE\") \\\n",
    "                   .agg(\n",
    "                       F.avg(\"AMOUNT\").alias(\"avg_top_ranked_amt\"),\n",
    "                       F.countDistinct(\"ORDER_ID\").alias(\"unique_top_orders\")\n",
    "                   ).orderBy(F.desc(\"avg_top_ranked_amt\"))\n",
    "\n",
    "result2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------+------------------+\n",
      "|SKU                     |max_velocity|avg_rolling_amt   |\n",
      "+------------------------+------------+------------------+\n",
      "|BL017-63BLACK           |8.000       |379.0             |\n",
      "|BL009-61BLACK           |3.000       |755.2777777777778 |\n",
      "|SET097-KR-PP-XXXL       |3.000       |1082.0            |\n",
      "|JNE3365-KR-1052-M       |3.000       |1128.0            |\n",
      "|JNE2305-KR-533-XXL      |2.714       |354.4927318295739 |\n",
      "|JNE2305-KR-533-L        |2.142       |339.91577964519144|\n",
      "|SET442-KR-NP-XXXL       |2.000       |1349.25           |\n",
      "|JNE3437-KR-XS           |2.000       |560.8755238095238 |\n",
      "|BTM039-PP-XXL           |2.000       |720.0             |\n",
      "|JNE1233-BLUE-KR-031-XXXL|2.000       |442.53714285714284|\n",
      "|JNE3503-KR-XXL          |2.000       |636.0             |\n",
      "|JNE3608-KR-M            |2.000       |397.8051282051282 |\n",
      "|J0005-DR-XXXL           |2.000       |931.8290017636684 |\n",
      "|SET268-KR-NP-XS         |2.000       |787.1046825396825 |\n",
      "|J0013-SKD-XXXL          |2.000       |1273.4804269293923|\n",
      "|JNE2265-KR-501-XXL      |2.000       |415.1301587301587 |\n",
      "|JNE3483-KR-XL           |2.000       |633.475           |\n",
      "|J0184-KR-S              |2.000       |NULL              |\n",
      "|JNE3674-TU-XXL          |2.000       |638.2888888888889 |\n",
      "|SET263-KR-NP-S          |2.000       |1228.0            |\n",
      "+------------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "df = df.withColumn(\"QTY\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"AMOUNT\", F.col(\"AMOUNT\").cast(\"double\"))\n",
    "\n",
    "w_rolling = Window.partitionBy(\"SKU\").orderBy(\"ORDER_DATE\").rowsBetween(-6, 0)\n",
    "\n",
    "df_rolling = df.withColumn(\"rolling_qty\", F.avg(\"QTY\").over(w_rolling)) \\\n",
    "               .withColumn(\"rolling_amt\", F.avg(\"AMOUNT\").over(w_rolling))\n",
    "\n",
    "result3 = df_rolling.groupBy(\"SKU\").agg(\n",
    "    F.max(\"rolling_qty\").alias(\"max_velocity\"),\n",
    "    F.avg(\"rolling_amt\").alias(\"avg_rolling_amt\")\n",
    ").orderBy(F.desc(\"max_velocity\"))\n",
    "\n",
    "result3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------+---------+---------------+-------------+-----------+\n",
      "|COURIER_STATUS               |delivered|cancelled|total_shipments|delivery_rate|cancel_rate|\n",
      "+-----------------------------+---------+---------+---------------+-------------+-----------+\n",
      "|Shipped - Picked Up          |0        |0        |973            |0.0          |0.0        |\n",
      "|Shipped - Delivered to Buyer |0        |0        |28769          |0.0          |0.0        |\n",
      "|Shipped - Out for Delivery   |0        |0        |35             |0.0          |0.0        |\n",
      "|Shipped - Damaged            |0        |0        |1              |0.0          |0.0        |\n",
      "|Pending                      |0        |2        |658            |0.0          |0.0        |\n",
      "|Shipping                     |0        |0        |8              |0.0          |0.0        |\n",
      "|Shipped - Lost in Transit    |0        |0        |5              |0.0          |0.0        |\n",
      "|Shipped - Returning to Seller|0        |0        |145            |0.0          |0.0        |\n",
      "|Shipped                      |0        |93       |77804          |0.0          |0.0        |\n",
      "|Cancelled                    |0        |5840     |18332          |0.0          |0.32       |\n",
      "|Pending - Waiting for Pick Up|0        |0        |281            |0.0          |0.0        |\n",
      "|Shipped - Rejected by Buyer  |0        |0        |11             |0.0          |0.0        |\n",
      "|Shipped - Returned to Seller |0        |0        |1953           |0.0          |0.0        |\n",
      "+-----------------------------+---------+---------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "df_flagged = df.withColumn(\"is_delivered\", F.col(\"STATUS\").like(\"%Delivered%\")) \\\n",
    "               .withColumn(\"is_cancelled\", F.col(\"STATUS\").like(\"%Cancelled%\"))\n",
    "\n",
    "result4 = df_flagged.groupBy(\"COURIER_STATUS\").agg(\n",
    "    F.sum(F.col(\"is_delivered\").cast(\"int\")).alias(\"delivered\"),\n",
    "    F.sum(F.col(\"is_cancelled\").cast(\"int\")).alias(\"cancelled\"),\n",
    "    F.count(\"*\").alias(\"total_shipments\")\n",
    ").withColumn(\"delivery_rate\", F.round(F.col(\"delivered\") / F.col(\"total_shipments\"), 2)) \\\n",
    " .withColumn(\"cancel_rate\", F.round(F.col(\"cancelled\") / F.col(\"total_shipments\"), 2)) \\\n",
    " .orderBy(F.desc(\"delivery_rate\"))\n",
    "\n",
    "result4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---+------------------+------------+-----------+\n",
      "|CATEGORY|SIZE|dow|avg_7d_total      |max_7d_total|avg_7d_qty |\n",
      "+--------+----+---+------------------+------------+-----------+\n",
      "|Blouse  |Free|Fri|2832.1470833333333|4086.0      |1.041458333|\n",
      "|Blouse  |Free|Mon|2740.14125        |3081.0      |0.886791667|\n",
      "|Blouse  |Free|Sat|2528.7400000000002|4066.0      |1.508956522|\n",
      "|Blouse  |Free|Sun|2754.6787096774196|4037.0      |0.962838710|\n",
      "|Blouse  |Free|Thu|2385.4806896551727|3058.9      |0.918310345|\n",
      "|Blouse  |Free|Tue|2608.7384         |2861.0      |0.879880000|\n",
      "|Blouse  |Free|Wed|2335.1148387096773|2907.0      |0.822064516|\n",
      "|Blouse  |L   |Fri|3417.735          |4241.0      |0.955312500|\n",
      "|Blouse  |L   |Mon|3897.2538888888894|4443.0      |0.960277778|\n",
      "|Blouse  |L   |Sat|3538.62875        |5107.0      |0.958291667|\n",
      "|Blouse  |L   |Sun|3603.3276470588235|4403.0      |0.949529412|\n",
      "|Blouse  |L   |Thu|3500.3153846153846|4509.0      |0.928500000|\n",
      "|Blouse  |L   |Tue|3773.2816666666663|4403.0      |0.934458333|\n",
      "|Blouse  |L   |Wed|3143.0654545454545|4067.0      |0.883000000|\n",
      "|Blouse  |M   |Fri|3714.108181818182 |4335.0      |0.870000000|\n",
      "|Blouse  |M   |Mon|3708.7595238095237|4400.33     |0.891047619|\n",
      "|Blouse  |M   |Sat|3789.294761904762 |4341.57     |0.911476190|\n",
      "|Blouse  |M   |Sun|3383.5642857142857|4995.0      |0.938714286|\n",
      "|Blouse  |M   |Thu|3539.5805405405404|5707.0      |0.911108108|\n",
      "|Blouse  |M   |Tue|3511.0430769230766|4606.0      |0.851500000|\n",
      "+--------+----+---+------------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "df = df.withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\"))\n",
    "\n",
    "w = Window.partitionBy(\"CATEGORY\", \"SIZE\").orderBy(\"ORDER_DATE\").rowsBetween(-6, 0)\n",
    "\n",
    "df_roll = df.withColumn(\"daily_total\", F.sum(\"amount\").over(w)) \\\n",
    "            .withColumn(\"daily_avg_qty\", F.avg(\"qty\").over(w)) \\\n",
    "            .withColumn(\"dow\", F.date_format(\"ORDER_DATE\", \"E\"))\n",
    "\n",
    "result1 = df_roll.groupBy(\"CATEGORY\", \"SIZE\", \"dow\").agg(\n",
    "    F.avg(\"daily_total\").alias(\"avg_7d_total\"),\n",
    "    F.max(\"daily_total\").alias(\"max_7d_total\"),\n",
    "    F.avg(\"daily_avg_qty\").alias(\"avg_7d_qty\")\n",
    ").orderBy(\"CATEGORY\", \"SIZE\", \"dow\")\n",
    "\n",
    "result1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+--------------+\n",
      "|ASIN|FULFILLED_BY|FULFILLED_BY|avg_price_diff|\n",
      "+----+------------+------------+--------------+\n",
      "+----+------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "df = df.withColumn(\"amount\", F.col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# Sample sub-dataframes\n",
    "df_fast = df.filter(F.col(\"ship_service_level\") == \"Standard\")\n",
    "df_slow = df.filter(F.col(\"ship_service_level\") != \"Standard\")\n",
    "\n",
    "# Join same table on ASIN to compare fulfillment speed\n",
    "df_joined = df_fast.alias(\"a\").join(\n",
    "    df_slow.alias(\"b\"),\n",
    "    on=F.col(\"a.ASIN\") == F.col(\"b.ASIN\"),\n",
    "    how=\"inner\"\n",
    ").select(\n",
    "    F.col(\"a.ASIN\"),\n",
    "    F.col(\"a.amount\").alias(\"amt_fast\"),\n",
    "    F.col(\"b.amount\").alias(\"amt_slow\"),\n",
    "    F.col(\"a.fulfilled_by\"),\n",
    "    F.col(\"b.fulfilled_by\")\n",
    ")\n",
    "a = df.filter(\"FULFILLED_BY = 'Easy Ship'\").select(\"ASIN\", \"FULFILLED_BY\", \"AMOUNT\").alias(\"a\")\n",
    "b = df.filter(\"FULFILLED_BY != 'Easy Ship'\").select(\"ASIN\", \"FULFILLED_BY\", \"AMOUNT\").alias(\"b\")\n",
    "\n",
    "df_joined = a.join(b, on=a[\"ASIN\"] == b[\"ASIN\"], how=\"inner\")\n",
    "\n",
    "result3 = df_joined.withColumn(\"price_diff\", b[\"AMOUNT\"] - a[\"AMOUNT\"]) \\\n",
    "    .groupBy(a[\"ASIN\"], a[\"FULFILLED_BY\"], b[\"FULFILLED_BY\"]) \\\n",
    "    .agg(F.avg(\"price_diff\").alias(\"avg_price_diff\")) \\\n",
    "    .orderBy(F.desc(\"avg_price_diff\"))\n",
    "\n",
    "result3.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------------+------------------+-------+------------------+----------+------------+\n",
      "|SHIP_STATE     |FULFILLED_BY|CATEGORY     |avg_top_amt       |max_amt|max_rolling_avg   |avg_promos|record_count|\n",
      "+---------------+------------+-------------+------------------+-------+------------------+----------+------------+\n",
      "|PUNJAB         |Easy Ship   |Set          |988.8482142857143 |5495.0 |2930.6666666666665|20.910714 |112         |\n",
      "|UTTAR PRADESH  |NULL        |Set          |987.9431034482759 |3036.0 |1669.0            |1.001724  |580         |\n",
      "|MAHARASHTRA    |NULL        |Set          |936.7288461538461 |2894.0 |1736.3333333333333|1.001923  |520         |\n",
      "|UTTAR PRADESH  |Easy Ship   |Western Dress|832.8450704225352 |2860.0 |2175.0            |20.936620 |142         |\n",
      "|HARYANA        |Easy Ship   |kurta        |556.3265306122449 |2796.0 |2796.0            |20.306122 |98          |\n",
      "|MAHARASHTRA    |NULL        |Western Dress|818.4285714285714 |2655.0 |1208.857142857143 |1.006494  |154         |\n",
      "|TELANGANA      |NULL        |Set          |932.5737704918033 |2598.0 |2598.0            |1.000000  |183         |\n",
      "|MAHARASHTRA    |Easy Ship   |Set          |971.2081081081081 |2598.0 |2598.0            |21.227027 |370         |\n",
      "|RAJASTHAN      |Easy Ship   |Set          |1030.2            |2598.0 |2710.86           |20.666667 |150         |\n",
      "|ANDHRA PRADESH |Easy Ship   |Set          |953.5117370892019 |2442.0 |2442.0            |21.075117 |213         |\n",
      "|TAMIL NADU     |NULL        |Set          |917.4825          |2397.0 |2372.0            |1.000000  |400         |\n",
      "|HARYANA        |Easy Ship   |Set          |1036.1268656716418|2372.0 |2372.0            |21.059701 |134         |\n",
      "|TAMIL NADU     |Easy Ship   |Set          |872.6403162055336 |2372.0 |1648.0            |20.592885 |253         |\n",
      "|Gujarat        |Easy Ship   |Set          |891.2078651685393 |2326.0 |1999.0            |21.123596 |178         |\n",
      "|Gujarat        |NULL        |Set          |987.4347826086956 |2326.0 |1698.0            |1.000000  |207         |\n",
      "|JAMMU & KASHMIR|NULL        |Set          |1001.0806451612904|2299.0 |2299.0            |1.000000  |62          |\n",
      "|KARNATAKA      |Easy Ship   |Set          |926.4933920704846 |2299.0 |1776.0            |20.731278 |227         |\n",
      "|ANDHRA PRADESH |NULL        |Set          |986.2470238095239 |2244.0 |1999.0            |1.000000  |336         |\n",
      "|KERALA         |Easy Ship   |Western Dress|762.819587628866  |2232.0 |2232.0            |20.827320 |388         |\n",
      "|HARYANA        |NULL        |Set          |1004.3457446808511|2224.0 |1698.0            |1.000000  |188         |\n",
      "+---------------+------------+-------------+------------------+-------+------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Load and preprocess\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "df = df.withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"promo_count\", F.size(F.split(\"PROMOTION_IDS\", \",\")))\n",
    "\n",
    "# Rolling window: 7-day per SKU and region\n",
    "rolling_window = Window.partitionBy(\"SKU\", \"SHIP_STATE\").orderBy(\"ORDER_DATE\").rowsBetween(-6, 0)\n",
    "\n",
    "df = df.withColumn(\"rolling_avg_amt\", F.avg(\"amount\").over(rolling_window)) \\\n",
    "       .withColumn(\"rolling_total_qty\", F.sum(\"qty\").over(rolling_window))\n",
    "\n",
    "# Ranking within city-category-fulfillment\n",
    "ranking_window = Window.partitionBy(\"SHIP_CITY\", \"CATEGORY\", \"FULFILLED_BY\").orderBy(F.desc(\"amount\"))\n",
    "\n",
    "df = df.withColumn(\"rank\", F.dense_rank().over(ranking_window))\n",
    "\n",
    "# Filter and group\n",
    "df_filtered = df.filter(\"rank <= 3 AND qty > 0 AND promo_count > 0\")\n",
    "\n",
    "# ‚úÖ FINAL: NO percentile_cont ‚Äî use MAX or AVG instead\n",
    "result = df_filtered.groupBy(\"SHIP_STATE\", \"FULFILLED_BY\", \"CATEGORY\").agg(\n",
    "    F.avg(\"amount\").alias(\"avg_top_amt\"),\n",
    "    F.max(\"amount\").alias(\"max_amt\"),  # Replacement for p95\n",
    "    F.max(\"rolling_avg_amt\").alias(\"max_rolling_avg\"),\n",
    "    F.avg(\"promo_count\").alias(\"avg_promos\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ").orderBy(F.desc(\"max_amt\"))\n",
    "\n",
    "result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+---------+-----------+-------+----------+\n",
      "|segment|ship_state|fulfilled_by|anomalies|avg_z_score|max_amt|avg_promos|\n",
      "+-------+----------+------------+---------+-----------+-------+----------+\n",
      "+-------+----------+------------+---------+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "import time\n",
    "# Load and preprocess\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "df = df.withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"promo_count\", F.size(F.split(\"PROMOTION_IDS\", \",\"))) \\\n",
    "       .withColumn(\"customer_id\", F.split(\"ORDER_ID\", \"-\")[1]) \\\n",
    "       .filter(\"amount IS NOT NULL AND qty > 0\")\n",
    "\n",
    "# Rest of logic is same as above...\n",
    "\n",
    "\n",
    "w_cust = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(-6, 0)\n",
    "\n",
    "df = df.withColumn(\"rolling_amt\", F.avg(\"amount\").over(w_cust)) \\\n",
    "       .withColumn(\"rolling_qty\", F.sum(\"qty\").over(w_cust))\n",
    "\n",
    "df = df.withColumn(\"z_score\", (F.col(\"amount\") - F.col(\"rolling_amt\")) / F.col(\"rolling_amt\"))\n",
    "\n",
    "cust_summary = df.groupBy(\"customer_id\").agg(\n",
    "    F.avg(\"amount\").alias(\"avg_spend\"),\n",
    "    F.count(\"*\").alias(\"order_count\")\n",
    ").withColumn(\n",
    "    \"segment\",\n",
    "    F.when((F.col(\"avg_spend\") > 1000) & (F.col(\"order_count\") > 10), \"high_value\")\n",
    "     .when((F.col(\"avg_spend\") < 300), \"low_value\")\n",
    "     .otherwise(\"medium_value\")\n",
    ")\n",
    "\n",
    "result = df.join(cust_summary, on=\"customer_id\", how=\"inner\") \\\n",
    "    .filter(\"z_score > 2.5\") \\\n",
    "    .groupBy(\"segment\", \"ship_state\", \"fulfilled_by\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"anomalies\"),\n",
    "        F.avg(\"z_score\").alias(\"avg_z_score\"),\n",
    "        F.max(\"amount\").alias(\"max_amt\"),\n",
    "        F.avg(\"promo_count\").alias(\"avg_promos\")\n",
    "    ).orderBy(F.desc(\"anomalies\"))\n",
    "\n",
    "result.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------+----------------+-----------+-----------------+----------------------+\n",
      "|cohort_month|category    |SHIP_STATE    |unique_customers|total_sales|avg_30d_spend    |repeat_category_buyers|\n",
      "+------------+------------+--------------+----------------+-----------+-----------------+----------------------+\n",
      "|0022-03     |blouse      |UTTAR PRADESH |1               |280.0      |280.0            |0                     |\n",
      "|0022-03     |ethnic dress|MAHARASHTRA   |1               |1099.0     |1099.0           |0                     |\n",
      "|0022-03     |kurta       |MAHARASHTRA   |12              |6485.0     |585.5833333333334|0                     |\n",
      "|0022-03     |kurta       |KARNATAKA     |11              |5521.43    |493.3691666666667|0                     |\n",
      "|0022-03     |kurta       |WEST BENGAL   |8               |4519.0     |552.0            |0                     |\n",
      "|0022-03     |kurta       |UTTAR PRADESH |9               |3697.0     |462.125          |0                     |\n",
      "|0022-03     |kurta       |TAMIL NADU    |7               |3438.95    |486.49375        |0                     |\n",
      "|0022-03     |kurta       |HARYANA       |3               |1955.19    |629.438          |0                     |\n",
      "|0022-03     |kurta       |Gujarat       |4               |1822.0     |427.8            |0                     |\n",
      "|0022-03     |kurta       |ODISHA        |2               |1181.0     |515.0            |0                     |\n",
      "|0022-03     |kurta       |KERALA        |3               |1146.0     |382.0            |0                     |\n",
      "|0022-03     |kurta       |ANDHRA PRADESH|2               |1137.0     |568.5            |0                     |\n",
      "|0022-03     |kurta       |BIHAR         |2               |1119.0     |559.5            |0                     |\n",
      "|0022-03     |kurta       |RAJASTHAN     |2               |814.0      |407.0            |0                     |\n",
      "|0022-03     |kurta       |TELANGANA     |1               |499.0      |499.0            |0                     |\n",
      "|0022-03     |kurta       |MADHYA PRADESH|1               |499.0      |499.0            |0                     |\n",
      "|0022-03     |kurta       |JHARKHAND     |1               |499.0      |499.0            |0                     |\n",
      "|0022-03     |kurta       |CHANDIGARH    |1               |353.0      |353.0            |0                     |\n",
      "|0022-03     |set         |MAHARASHTRA   |11              |8953.0     |895.3            |0                     |\n",
      "|0022-03     |set         |KARNATAKA     |9               |7215.0     |896.0            |0                     |\n",
      "+------------+------------+--------------+----------------+-----------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "import time\n",
    "\n",
    "# Load Snowflake data\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "df = df.withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"customer_id\", F.split(\"ORDER_ID\", \"-\")[1]) \\\n",
    "       .withColumn(\"month\", F.date_format(\"ORDER_DATE\", \"yyyy-MM\")) \\\n",
    "       .withColumn(\"category\", F.lower(F.col(\"CATEGORY\")))\n",
    "\n",
    "# Step 2: Get cohort month (first order date per customer)\n",
    "w_first = Window.partitionBy(\"customer_id\").orderBy(\"ORDER_DATE\")\n",
    "df = df.withColumn(\"first_order_date\", F.first(\"ORDER_DATE\").over(w_first)) \\\n",
    "       .withColumn(\"cohort_month\", F.date_format(\"first_order_date\", \"yyyy-MM\"))\n",
    "\n",
    "# Step 3: Rolling 30-row window spend (row-based, not time-based)\n",
    "w_rolling = Window.partitionBy(\"customer_id\").orderBy(\"ORDER_DATE\").rowsBetween(-30, 0)\n",
    "df = df.withColumn(\"rolling_30d_amt\", F.sum(\"amount\").over(w_rolling))\n",
    "\n",
    "# Step 4: Repurchase detection (if a customer bought from a category in >1 month)\n",
    "repurchase_flag = df.groupBy(\"customer_id\", \"category\") \\\n",
    "                    .agg(F.countDistinct(\"month\").alias(\"active_months\")) \\\n",
    "                    .withColumn(\"repurchase\", F.expr(\"active_months > 1\"))\n",
    "\n",
    "# Step 5: Join repurchase flag back\n",
    "df = df.join(repurchase_flag, on=[\"customer_id\", \"category\"], how=\"left\")\n",
    "\n",
    "# Step 6: Aggregate metrics by cohort, category, and region\n",
    "result = df.groupBy(\"cohort_month\", \"category\", \"SHIP_STATE\").agg(\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    F.sum(\"amount\").alias(\"total_sales\"),\n",
    "    F.avg(\"rolling_30d_amt\").alias(\"avg_30d_spend\"),\n",
    "    F.sum(F.col(\"repurchase\").cast(\"int\")).alias(\"repeat_category_buyers\")\n",
    ").orderBy(\"cohort_month\", \"category\", F.desc(\"total_sales\"))\n",
    "\n",
    "# Step 7: Execute and benchmark\n",
    "start = time.time()\n",
    "result.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------+--------+-----------------+------------------+--------------+------------------+\n",
      "|SHIP_STATE     |FULFILLED_BY|category|top_skus|total_margin     |avg_margin        |price_outliers|avg_unit_price    |\n",
      "+---------------+------------+--------+--------+-----------------+------------------+--------------+------------------+\n",
      "|WEST BENGAL    |Easy Ship   |set     |10      |8374.800000000001|644.2153846153847 |0             |1534.5384615384614|\n",
      "|BIHAR          |NULL        |set     |12      |8240.400000000001|633.8769230769232 |0             |1584.6923076923076|\n",
      "|MADHYA PRADESH |NULL        |set     |10      |8078.0           |621.3846153846154 |1             |1553.4615384615386|\n",
      "|KARNATAKA      |Easy Ship   |set     |9       |8032.000000000001|669.3333333333334 |0             |1533.3333333333333|\n",
      "|MADHYA PRADESH |Easy Ship   |set     |11      |7473.200000000001|622.7666666666668 |1             |1556.9166666666667|\n",
      "|RAJASTHAN      |Easy Ship   |set     |10      |7136.400000000001|713.6400000000001 |0             |1546.9            |\n",
      "|PUNJAB         |Easy Ship   |set     |6       |6396.0           |913.7142857142857 |1             |1499.2857142857142|\n",
      "|ODISHA         |NULL        |set     |8       |6290.0           |698.8888888888889 |0             |1528.3333333333333|\n",
      "|ODISHA         |Easy Ship   |set     |8       |6134.400000000001|613.44            |0             |1533.6            |\n",
      "|JAMMU & KASHMIR|NULL        |set     |9       |5922.400000000001|658.0444444444445 |0             |1521.2222222222222|\n",
      "|ASSAM          |Easy Ship   |set     |9       |5877.6           |587.76            |0             |1469.4            |\n",
      "|HARYANA        |NULL        |set     |7       |5801.200000000001|725.1500000000001 |0             |1534.875          |\n",
      "|MAHARASHTRA    |Easy Ship   |set     |6       |5667.6           |944.6             |0             |1924.5            |\n",
      "|NAGALAND       |NULL        |set     |7       |5566.000000000001|695.7500000000001 |0             |1046.625          |\n",
      "|ANDHRA PRADESH |NULL        |set     |5       |5431.6           |1086.3200000000002|2             |1077.8            |\n",
      "|CHHATTISGARH   |NULL        |set     |8       |5411.6           |601.2888888888889 |1             |1503.2222222222222|\n",
      "|JAMMU & KASHMIR|Easy Ship   |set     |11      |5411.2           |491.92727272727274|0             |1229.8181818181818|\n",
      "|TELANGANA      |NULL        |set     |6       |5038.8           |839.8000000000001 |0             |1579.0            |\n",
      "|DELHI          |Easy Ship   |set     |7       |4966.8           |620.85            |1             |1552.125          |\n",
      "|BIHAR          |Easy Ship   |set     |7       |4918.400000000001|614.8000000000001 |0             |1537.0            |\n",
      "+---------------+------------+--------+--------+-----------------+------------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "df = df.withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"unit_price\", (F.col(\"amount\") / F.col(\"qty\")).cast(\"double\")) \\\n",
    "       .withColumn(\"category\", F.lower(F.col(\"CATEGORY\"))) \\\n",
    "       .filter(\"qty > 0 AND amount > 0 AND unit_price IS NOT NULL\")\n",
    "\n",
    "# Step 2: Fulfillment + SKU window stats\n",
    "w_sku_fulfill = Window.partitionBy(\"SKU\", \"FULFILLED_BY\")\n",
    "\n",
    "df_stats = df.withColumn(\"avg_unit_price\", F.avg(\"unit_price\").over(w_sku_fulfill)) \\\n",
    "             .withColumn(\"stddev_unit_price\", F.stddev(\"unit_price\").over(w_sku_fulfill)) \\\n",
    "             .withColumn(\"z_score\", (F.col(\"unit_price\") - F.col(\"avg_unit_price\")) / F.col(\"stddev_unit_price\"))\n",
    "\n",
    "# Step 3: Flag potential price outliers\n",
    "df_stats = df_stats.withColumn(\"is_outlier\", (F.abs(F.col(\"z_score\")) > 2).cast(\"int\"))\n",
    "\n",
    "# Step 4: Simulate margin (assume static COGS for demo)\n",
    "df_stats = df_stats.withColumn(\"COGS\", (F.col(\"unit_price\") * 0.6).cast(\"double\")) \\\n",
    "                   .withColumn(\"gross_margin\", (F.col(\"unit_price\") - F.col(\"COGS\")) * F.col(\"qty\"))\n",
    "\n",
    "# Step 5: SKU rank by margin contribution\n",
    "w_rank = Window.partitionBy(\"SHIP_STATE\", \"FULFILLED_BY\").orderBy(F.desc(\"gross_margin\"))\n",
    "df_stats = df_stats.withColumn(\"sku_rank\", F.dense_rank().over(w_rank))\n",
    "\n",
    "# Step 6: Final aggregation\n",
    "result = df_stats.filter(\"sku_rank <= 5\").groupBy(\"SHIP_STATE\", \"FULFILLED_BY\", \"category\").agg(\n",
    "    F.countDistinct(\"SKU\").alias(\"top_skus\"),\n",
    "    F.sum(\"gross_margin\").alias(\"total_margin\"),\n",
    "    F.avg(\"gross_margin\").alias(\"avg_margin\"),\n",
    "    F.sum(\"is_outlier\").alias(\"price_outliers\"),\n",
    "    F.avg(\"unit_price\").alias(\"avg_unit_price\")\n",
    ").orderBy(F.desc(\"total_margin\"))\n",
    "\n",
    "# Step 7: Execute\n",
    "result.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+------------+-------------+---------------+------------+----------------+-----------------+--------+-----------------+\n",
      "|SHIP_STATE       |sales_channel|FULFILLED_BY|category     |loyalty_segment|total_orders|unique_customers|avg_order_value  |avg_qty |avg_lifetime_days|\n",
      "+-----------------+-------------+------------+-------------+---------------+------------+----------------+-----------------+--------+-----------------+\n",
      "|NULL             |Amazon.in    |Easy Ship   |set          |newcomer       |6           |6               |801.6666666666666|1.000000|0.000000         |\n",
      "|NULL             |Amazon.in    |Easy Ship   |western dress|newcomer       |1           |1               |735.0            |1.000000|0.000000         |\n",
      "|NULL             |Amazon.in    |Easy Ship   |kurta        |newcomer       |3           |3               |373.3333333333333|1.000000|0.000000         |\n",
      "|NULL             |Amazon.in    |NULL        |top          |newcomer       |2           |2               |490.0            |1.000000|0.000000         |\n",
      "|NULL             |Amazon.in    |NULL        |set          |newcomer       |5           |4               |827.6            |1.000000|0.000000         |\n",
      "|NULL             |Amazon.in    |NULL        |kurta        |newcomer       |11          |10              |522.5454545454545|1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |set          |churn_risk     |1           |1               |1221.0           |1.000000|730414.000000    |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |kurta        |newcomer       |22          |21              |384.6363636363636|1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |bottom       |newcomer       |1           |1               |518.0            |1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |set          |newcomer       |26          |24              |799.8076923076923|1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |NULL        |kurta        |newcomer       |61          |55              |442.8524590163934|1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |NULL        |western dress|newcomer       |7           |7               |768.1428571428571|1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |top          |newcomer       |5           |5               |544.2            |1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |NULL        |set          |newcomer       |77          |74              |844.0            |1.038961|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |NULL        |top          |newcomer       |5           |5               |617.4            |1.000000|0.000000         |\n",
      "|ANDAMAN & NICOBAR|Amazon.in    |Easy Ship   |western dress|newcomer       |16          |15              |744.375          |1.000000|0.000000         |\n",
      "|ANDHRA PRADESH   |Amazon.in    |Easy Ship   |set          |churn_risk     |5           |5               |939.2            |1.000000|438304.800000    |\n",
      "|ANDHRA PRADESH   |Amazon.in    |Easy Ship   |kurta        |churn_risk     |5           |5               |422.4            |1.000000|438316.800000    |\n",
      "|ANDHRA PRADESH   |Amazon.in    |Easy Ship   |western dress|churn_risk     |4           |4               |850.0            |1.000000|547865.000000    |\n",
      "|ANDHRA PRADESH   |Amazon.in    |NULL        |set          |churn_risk     |10          |10              |763.2            |1.000000|292222.100000    |\n",
      "+-----------------+-------------+------------+-------------+---------------+------------+----------------+-----------------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Step 1: Load from Snowflake and fix schema issues\n",
    "df = spark.read.table(\"CORTEX_AGENTS_DEMO.PUBLIC.ORDERS\")\n",
    "\n",
    "# Fix column names and cast types\n",
    "df = df.withColumnRenamed(\"SALES_CHANNEL \", \"sales_channel\") \\\n",
    "       .withColumn(\"amount\", F.col(\"AMOUNT\").cast(\"double\")) \\\n",
    "       .withColumn(\"qty\", F.col(\"QTY\").cast(\"int\")) \\\n",
    "       .withColumn(\"order_date\", F.col(\"ORDER_DATE\").cast(\"date\")) \\\n",
    "       .withColumn(\"customer_id\", F.split(F.col(\"ORDER_ID\"), \"-\")[1]) \\\n",
    "       .withColumn(\"category\", F.lower(F.col(\"CATEGORY\"))) \\\n",
    "       .withColumn(\"ship_speed\", F.col(\"SHIP_SERVICE_LEVEL\")) \\\n",
    "       .filter(\"qty > 0 AND amount > 0\")\n",
    "\n",
    "# Step 2: Add cohort data\n",
    "w_cust = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "df = df.withColumn(\"first_order\", F.first(\"order_date\").over(w_cust)) \\\n",
    "       .withColumn(\"last_order\", F.last(\"order_date\").over(w_cust))\n",
    "\n",
    "# Step 3: Calculate frequency metrics per customer\n",
    "df_freq = df.groupBy(\"customer_id\").agg(\n",
    "    F.countDistinct(\"category\").alias(\"unique_categories\"),\n",
    "    F.countDistinct(F.date_format(\"order_date\", \"yyyy-MM\")).alias(\"active_months\"),\n",
    "    F.datediff(F.max(\"order_date\"), F.min(\"order_date\")).alias(\"lifetime_days\")\n",
    ")\n",
    "\n",
    "# Step 4: Segment customers by loyalty\n",
    "df_freq = df_freq.withColumn(\"loyalty_segment\", \n",
    "    F.when((F.col(\"unique_categories\") == 1) & (F.col(\"active_months\") > 2), \"loyalist\")\n",
    "     .when((F.col(\"unique_categories\") > 2) & (F.col(\"active_months\") > 2), \"switcher\")\n",
    "     .when(F.col(\"lifetime_days\") < 30, \"newcomer\")\n",
    "     .otherwise(\"churn_risk\")\n",
    ")\n",
    "\n",
    "# Step 5: Join back with the original dataset\n",
    "df_full = df.join(df_freq, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "# Step 6: Aggregation by dimensions\n",
    "result = df_full.groupBy(\"SHIP_STATE\", \"sales_channel\", \"FULFILLED_BY\", \"category\", \"loyalty_segment\").agg(\n",
    "    F.count(\"*\").alias(\"total_orders\"),\n",
    "    F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    F.avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    F.avg(\"qty\").alias(\"avg_qty\"),\n",
    "    F.avg(\"lifetime_days\").alias(\"avg_lifetime_days\")\n",
    ").orderBy(\"SHIP_STATE\", \"sales_channel\", \"loyalty_segment\")\n",
    "\n",
    "# Show result\n",
    "result.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
